{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7H2lhj6gLyRU"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeJhO9RPLyRZ"
   },
   "source": [
    "# Small data and deep learning\n",
    "This mini-project proposes to study several techniques for improving challenging context, in which few data and resources are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-ZDU0bjLyRa"
   },
   "source": [
    "# Introduction\n",
    "Assume we are in a context where few \"gold\" labeled data are available for training, say $\\mathcal{X}_{\\text{train}}\\triangleq\\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$, where $N_{\\text{train}}$ is small. A large test set $\\mathcal{X}_{\\text{test}}$ is available. A large amount of unlabeled data, $\\mathcal{X}$, is available. We also assume that we have a limited computational budget (e.g., no GPUs).\n",
    "\n",
    "For each question, write a commented *Code* or a complete answer as a *Markdown*. When the objective of a question is to report a CNN accuracy, please use the following format to report it, at the end of the question:\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
    "|------|------|------|------|\n",
    "|   XXX  | XXX | XXX | XXX |\n",
    "\n",
    "If applicable, please add the field corresponding to the  __Accuracy on Full Data__ as well as a link to the __Reference paper__ you used to report those numbers. (You do not need to train a CNN on the full CIFAR10 dataset)\n",
    "\n",
    "In your final report, please keep the logs of each training procedure you used. We will only run this jupyter if we have some doubts on your implementation. \n",
    "\n",
    "__The total file sizes should not exceed 2MB. Please name your notebook (LASTNAME)\\_(FIRSTNAME).ipynb, zip/tar it with any necessary files required to run your notebook, in a compressed file named (LASTNAME)\\_(FIRSTNAME).X where X is the corresponding extension. Zip/tar files exceeding 2MB will not be considered for grading. Submit the compressed file via the submission link provided on the website of the class.__\n",
    "\n",
    "You can use https://colab.research.google.com/ to run your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b60U5du3LyRb"
   },
   "source": [
    "## Training set creation\n",
    "__Question 1:__ Propose a dataloader or modify the file located at https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py in order to obtain a training loader that will only use the first 100 samples of the CIFAR-10 training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gk5ETsCyLyRc"
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mAGYF_H3LyRd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQmCidX2LyRg"
   },
   "source": [
    "### Import downloader behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "liXruxpqLyRh"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sys\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import hashlib\n",
    "import errno\n",
    "from torch.utils.model_zoo import tqdm\n",
    "\n",
    "## UTILS FUNCTION from https://github.com/pytorch/vision/blob/master/torchvision/datasets/utils.py\n",
    "\n",
    "def check_integrity(fpath, md5=None):\n",
    "    if md5 is None:\n",
    "        return True\n",
    "    if not os.path.isfile(fpath):\n",
    "        return False\n",
    "    md5o = hashlib.md5()\n",
    "    with open(fpath, 'rb') as f:\n",
    "        # read in 1MB chunks\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b''):\n",
    "            md5o.update(chunk)\n",
    "    md5c = md5o.hexdigest()\n",
    "    if md5c != md5:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def download_url(url, root, filename=None, md5=None):\n",
    "    \"\"\"Download a file from a url and place it in root.\n",
    "    Args:\n",
    "        url (str): URL to download file from\n",
    "        root (str): Directory to place downloaded file in\n",
    "        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n",
    "        md5 (str, optional): MD5 checksum of the download. If None, do not check\n",
    "    \"\"\"\n",
    "    from six.moves import urllib\n",
    "\n",
    "    root = os.path.expanduser(root)\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "    fpath = os.path.join(root, filename)\n",
    "\n",
    "    makedir_exist_ok(root)\n",
    "\n",
    "    # downloads file\n",
    "    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n",
    "        print('Using downloaded and verified file: ' + fpath)\n",
    "    else:\n",
    "        try:\n",
    "            print('Downloading ' + url + ' to ' + fpath)\n",
    "            urllib.request.urlretrieve(\n",
    "                url, fpath,\n",
    "                reporthook=gen_bar_updater()\n",
    "            )\n",
    "        except OSError:\n",
    "            if url[:5] == 'https':\n",
    "                url = url.replace('https:', 'http:')\n",
    "                print('Failed download. Trying https -> http instead.'\n",
    "                      ' Downloading ' + url + ' to ' + fpath)\n",
    "                urllib.request.urlretrieve(\n",
    "                    url, fpath,\n",
    "                    reporthook=gen_bar_updater()\n",
    "                )\n",
    "\n",
    "def makedir_exist_ok(dirpath):\n",
    "    \"\"\"\n",
    "    Python2 support for os.makedirs(.., exist_ok=True)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except OSError as e:\n",
    "        if e.errno == errno.EEXIST:\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "def gen_bar_updater():\n",
    "    pbar = tqdm(total=None)\n",
    "\n",
    "    def bar_update(count, block_size, total_size):\n",
    "        if pbar.total is None and total_size:\n",
    "            pbar.total = total_size\n",
    "        progress_bytes = count * block_size\n",
    "        pbar.update(progress_bytes - pbar.n)\n",
    "\n",
    "    return bar_update\n",
    "\n",
    "## CIFAR DATASET DOWNLOADER from https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py\n",
    "    \n",
    "class CIFAR10(data.Dataset):\n",
    "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
    "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
    "            creates from test set.\n",
    "        transform (callable, optional): A function/transform that takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-10-batches-py'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    filename = \"cifar-10-python.tar.gz\"\n",
    "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "    train_list = [\n",
    "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
    "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
    "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
    "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
    "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'batches.meta',\n",
    "        'key': 'label_names',\n",
    "        'md5': '5ff9c542aee3614f3951f8cda6e48888',\n",
    "    }\n",
    "\n",
    "    def __init__(self, root, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        if self.train:\n",
    "            downloaded_list = self.train_list\n",
    "        else:\n",
    "            downloaded_list = self.test_list\n",
    "\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "\n",
    "        # now load the picked numpy arrays\n",
    "        for file_name, checksum in downloaded_list:\n",
    "            file_path = os.path.join(self.root, self.base_folder, file_name)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    entry = pickle.load(f)\n",
    "                else:\n",
    "                    entry = pickle.load(f, encoding='latin1')\n",
    "                self.data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.targets.extend(entry['labels'])\n",
    "                else:\n",
    "                    self.targets.extend(entry['fine_labels'])\n",
    "\n",
    "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
    "        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "\n",
    "        self._load_meta()\n",
    "\n",
    "    def _load_meta(self):\n",
    "        path = os.path.join(self.root, self.base_folder, self.meta['filename'])\n",
    "        if not check_integrity(path, self.meta['md5']):\n",
    "            raise RuntimeError('Dataset metadata file not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "        with open(path, 'rb') as infile:\n",
    "            if sys.version_info[0] == 2:\n",
    "                data = pickle.load(infile)\n",
    "            else:\n",
    "                data = pickle.load(infile, encoding='latin1')\n",
    "            self.classes = data[self.meta['key']]\n",
    "        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        root = self.root\n",
    "        for fentry in (self.train_list + self.test_list):\n",
    "            filename, md5 = fentry[0], fentry[1]\n",
    "            fpath = os.path.join(root, self.base_folder, filename)\n",
    "            if not check_integrity(fpath, md5):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def download(self):\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        download_url(self.url, self.root, self.filename, self.tgz_md5)\n",
    "\n",
    "        # extract file\n",
    "        with tarfile.open(os.path.join(self.root, self.filename), \"r:gz\") as tar:\n",
    "            tar.extractall(path=self.root)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = 'train' if self.train is True else 'test'\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "\n",
    "class CIFAR100(CIFAR10):\n",
    "    \"\"\"`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    This is a subclass of the `CIFAR10` Dataset.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-100-python'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
    "    train_list = [\n",
    "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'meta',\n",
    "        'key': 'fine_label_names',\n",
    "        'md5': '7973b15100ade9c7d40fb424638fde48',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJqbeSk7LyRk"
   },
   "source": [
    "### Run test downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVl9FQvyLyRl"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset\"\n",
    "TRAIN_BATCH_PATH = \"/\".join([DATASET_PATH, \"cifar-10-batches-py\", \"data_batch_1\"])\n",
    "TEST_BATCH_PATH = \"/\".join([DATASET_PATH, \"cifar-10-batches-py\", \"test_batch\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twjlIeo8LyRn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download train dataset\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "print(\"Download train dataset\")\n",
    "downloader_train = CIFAR10(root=DATASET_PATH, train=True, transform=None, target_transform=None, download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXmv8_CGLyRq"
   },
   "source": [
    "### Unpack the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9uBc3Zi4LyRr"
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "train_data_dict = unpickle(TRAIN_BATCH_PATH)\n",
    "test_data_dict = unpickle(TEST_BATCH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8q1Jg_9SLyRv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (100, 32, 32, 3)\n",
      "X_left shape (9900, 32, 32, 3)\n",
      "X_test shape (10000, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def custom_dataset_process(data_dict, n_start = 0, n_end=None):\n",
    "    \"\"\" Load the CIFAR10 data as intended for this project\n",
    "    :param data_dict: dict, dictionnary generated by the unpickle function\n",
    "    :param n_start: integer, default=None, index value of the first elements to take\n",
    "    :param n_end: integer, default=None, index value of the last elements to take\n",
    "    :return x: np.array, data array with shape n*32*32*3, with n number of images, of shape 32*32 in 3 colors\"\"\"\n",
    "    \n",
    "    labels = data_dict[b\"labels\"][n_start:n_end]\n",
    "    data = data_dict[b\"data\"][n_start:n_end]\n",
    "        \n",
    "    les_im = np.array(data)\n",
    "    les_im_reshape = []\n",
    "    for id_im, im in enumerate(les_im):\n",
    "        im_reshape = np.rot90(im.reshape((32, 32, 3), order=\"F\"), k=3)\n",
    "        les_im_reshape.append(im_reshape)\n",
    "    \n",
    "    les_im_reshape = np.array(les_im_reshape)\n",
    "    return les_im_reshape, labels\n",
    "\n",
    "X_train, X_train_labels = custom_dataset_process(train_data_dict, n_start=0, n_end=100)\n",
    "X_left, X_left_labels = custom_dataset_process(train_data_dict, n_start=100, n_end=None)\n",
    "X_test, X_test_labels = custom_dataset_process(test_data_dict)\n",
    "\n",
    "def show_dataset():\n",
    "    for X in [[\"X_train\", X_train], [\"X_left\", X_left], [\"X_test\", X_test]]:\n",
    "        print(X[0], \"shape\", X[1].shape)\n",
    "        plt.figure()\n",
    "        plt.imshow(X[1][1])\n",
    "\n",
    "show_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8siNgFNLyRz"
   },
   "source": [
    "This is our dataset $\\mathcal{X}_{\\text{train}}$, it will be used until the end of this project. The remaining samples correspond to $\\mathcal{X}$. The testing set $\\mathcal{X}_{\\text{test}}$ corresponds to the whole testing set of CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXn5rOGkLyR3"
   },
   "source": [
    "## Testing procedure\n",
    "__Question 2:__ Explain why the evaluation of the training procedure is difficult. Propose several solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0o251Kz8LyR4"
   },
   "source": [
    "Evaluation of the training procedure may be difficult because of the low number of labeled data we are using for the model training. We may have quite a hard time resolving overfitting problems: we have few gold data.\n",
    "\n",
    "As seen in the class, solutions exists:\n",
    "- We can use semi-supervized learning to try to make guesses about unlabeled train data\n",
    "- Use transfer learning, and just fine-tune our model without the need of a huge great dataset\n",
    "- Weak supervision or traditional supervision are also existing solutions, but we will not use them as they involve to ask for a new labelling round, even at higher/abstract level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tp4Hh9BoLyR5"
   },
   "source": [
    "# Raw approach: the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsyOxEtxLyR6"
   },
   "source": [
    "In this section, the goal is to train a CNN on $\\mathcal{X}_{\\text{train}}$ and compare its performances with reported number from the litterature. You will have to re-use and/or design a standard classification pipeline. You should optimize your pipeline to obtain the best performances (image size, data augmentation by flip, ...).\n",
    "\n",
    "The key ingredients for training a CNN are the batch size, as well as the learning rate schedule, i.e. how to decrease the learning rate as a function of the number of epochs. A possible schedule is to start the learning rate at 0.1 and decreasing it every 30 epochs by 10. In case of divergence, reduce the laerning rate. A potential batch size could be 10, yet this can be cross-validated.\n",
    "\n",
    "You can get some baselines accuracies in this paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf. Obviously, it is a different context, as those researchers had access to GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-C45ofQLyR7"
   },
   "source": [
    "## ResNet architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tv78YLvaLyR8"
   },
   "source": [
    "__Question 3:__ Write a classification pipeline for $\\mathcal{X}_{\\text{train}}$, train from scratch and evaluate a *ResNet-18* architecture specific to CIFAR10 (details about the ImageNet model can be found here: https://arxiv.org/abs/1409.1556 ). If possible, please report the accuracy obtained on the whole dataset, as well as the reference paper/GitHub link you might have used.\n",
    "\n",
    "*Hint:* You can re-use the following code: https://github.com/kuangliu/pytorch-cifar. During a training of 10 epochs, a batch size of 10 and a learning rate of 0.01, one obtains 40% accuracy on $\\mathcal{X}_{\\text{train}}$ (~2 minutes) and 20% accuracy on $\\mathcal{X}_{\\text{test}}$ (~5 minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fs3ORUL3STtF"
   },
   "source": [
    "### Kuangliu implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSxNxKF8LyR9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "'''ResNet in PyTorch.\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1,3,32,32))\n",
    "    print(y.size())\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E8QdVqzwLyR_"
   },
   "source": [
    "#### Utils : https://github.com/kuangliu/pytorch-cifar/blob/master/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zijagxanLySA"
   },
   "outputs": [],
   "source": [
    "'''Some helper functions for PyTorch, including:\n",
    "    - get_mean_and_std: calculate the mean and std value of dataset.\n",
    "    - msr_init: net parameter initialization.\n",
    "    - progress_bar: progress bar mimic xlua.progress.\n",
    "'''\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "def get_mean_and_std(dataset):\n",
    "    '''Compute the mean and std value of dataset.'''\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    print('==> Computing mean and std..')\n",
    "    for inputs, targets in dataloader:\n",
    "        for i in range(3):\n",
    "            mean[i] += inputs[:,i,:,:].mean()\n",
    "            std[i] += inputs[:,i,:,:].std()\n",
    "    mean.div_(len(dataset))\n",
    "    std.div_(len(dataset))\n",
    "    return mean, std\n",
    "\n",
    "def init_params(net):\n",
    "    '''Init layer parameters.'''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal(m.weight, mode='fan_out')\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant(m.weight, 1)\n",
    "            init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal(m.weight, std=1e-3)\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "\n",
    "\n",
    "# _, term_width = os.popen('stty size', 'r').read().split()\n",
    "# term_width = int(term_width)\n",
    "term_width = 1\n",
    "TOTAL_BAR_LENGTH = 65.\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global last_time, begin_time\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
    "\n",
    "    if current < total-1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    secondsf = int(seconds)\n",
    "    seconds = seconds - secondsf\n",
    "    millis = int(seconds*1000)\n",
    "\n",
    "    f = ''\n",
    "    i = 1\n",
    "    if days > 0:\n",
    "        f += str(days) + 'D'\n",
    "        i += 1\n",
    "    if hours > 0 and i <= 2:\n",
    "        f += str(hours) + 'h'\n",
    "        i += 1\n",
    "    if minutes > 0 and i <= 2:\n",
    "        f += str(minutes) + 'm'\n",
    "        i += 1\n",
    "    if secondsf > 0 and i <= 2:\n",
    "        f += str(secondsf) + 's'\n",
    "        i += 1\n",
    "    if millis > 0 and i <= 2:\n",
    "        f += str(millis) + 'ms'\n",
    "        i += 1\n",
    "    if f == '':\n",
    "        f = '0ms'\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GklFJQcXLySD"
   },
   "source": [
    "#### Adapted from main https://github.com/kuangliu/pytorch-cifar/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0qfE7HvLySD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/paul/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/paul/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/paul/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/paul/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paul/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/paul/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/paul/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/paul/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ba839311d010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-ba839311d010>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, lr, try_cuda):\n",
    "        self.lr = lr\n",
    "        self.try_cuda = try_cuda\n",
    "\n",
    "args = Args(lr=0.1, try_cuda=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() and args.try_cuda else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=DATASET_PATH, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=DATASET_PATH, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "# net = VGG('VGG19')\n",
    "net = ResNet18()\n",
    "# net = PreActResNet18()\n",
    "# net = GoogLeNet()\n",
    "# net = DenseNet121()\n",
    "# net = ResNeXt29_2x64d()\n",
    "# net = MobileNet()\n",
    "# net = MobileNetV2()\n",
    "# net = DPN92()\n",
    "# net = ShuffleNetG2()\n",
    "# net = SENet18()\n",
    "# net = ShuffleNetV2(1)\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+200):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7iDpQVyBLySG"
   },
   "source": [
    "## VGG-like architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwkHhRzKLySH"
   },
   "source": [
    "__Question 4:__ Same question as before, but with a *VGG*. Which model do you recommend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0UnhQVqLySI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Hij8vN4LySL"
   },
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ta0dVVkLLySM"
   },
   "source": [
    "We propose to use pre-trained models on a classification and generative task, in order to improve the results of our setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pf28XHgMLySO"
   },
   "source": [
    "## ImageNet features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yO8O4EBLySP"
   },
   "source": [
    "Now, we will use some pre-trained models on ImageNet and see how well they compare on CIFAR. A list is available on: https://pytorch.org/docs/stable/torchvision/models.html.\n",
    "\n",
    "__Question 5:__ Pick a model from the list above, adapt it to CIFAR and retrain its final layer (or a block of layers, depending on the resources to which you have access to). Report its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JeHJQDBhLySS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_names ['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'inception_v3', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 6 and 9 in dimension 1 at /opt/conda/conda-bld/pytorch_1549635019666/work/aten/src/TH/generic/THTensorMoreMath.cpp:1307",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6ce15c3d00c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-6ce15c3d00c6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Transform into torch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mX_train_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mX_train_labels_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 6 and 9 in dimension 1 at /opt/conda/conda-bld/pytorch_1549635019666/work/aten/src/TH/generic/THTensorMoreMath.cpp:1307"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "print(\"model_names\", model_names)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "parser.add_argument('data', metavar='DIR',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet18',\n",
    "                    choices=model_names,\n",
    "                    help='model architecture: ' +\n",
    "                        ' | '.join(model_names) +\n",
    "                        ' (default: resnet18)')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--epochs', default=90, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 256)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                    metavar='N', help='print frequency (default: 10)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                    help='evaluate model on validation set')\n",
    "parser.add_argument('--pretrained', dest='pretrained', action='store_true',\n",
    "                    help='use pre-trained model')\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, data=\"dataset\", arch=\"resnet_18\", workers=4, epochs=99, start_epoch=0, lr=0.1, momentum=0.9, weight_decay=1e-4, resume=\"\", evaluate=True, pretrained=True):\n",
    "      self.data = data\n",
    "      self.arch = arch\n",
    "      self.workers = workers\n",
    "      self.epochs = epochs\n",
    "      self.start_epoch = start_epoch\n",
    "      self.lr = lr\n",
    "      self.momentum = momentum\n",
    "      self.weight_decay = weight_decay\n",
    "      self.resume = resume\n",
    "      self.evaluate = evaluate\n",
    "      self.pretrained = pretrained\n",
    "\n",
    "\n",
    "best_prec1 = 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    global args, best_prec1\n",
    "#     args = parser.parse_args()\n",
    "    args = Args()\n",
    "  \n",
    "    ## Dataset adaptation to torch \n",
    "    # Transform into torch tensor\n",
    "    X_train_torch = torch.from_numpy(X_train)\n",
    "    X_train_labels_torch = torch.from_numpy(X_tr\n",
    "    \n",
    "    \n",
    "    # create model\n",
    "    if args.pretrained:\n",
    "        print(\"=> using pre-trained model '{}'\".format(args.arch))\n",
    "        model = models.__dict__[args.arch](pretrained=True)\n",
    "    else:\n",
    "        print(\"=> creating model '{}'\".format(args.arch))\n",
    "        model = models.__dict__[args.arch]()\n",
    "\n",
    "    if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):\n",
    "        model.features = torch.nn.DataParallel(model.features)\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Data loading code\n",
    "#     traindir = os.path.join(args.data, 'train')\n",
    "#     valdir = os.path.join(args.data, 'val')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.ImageFolder(traindir, transforms.Compose([\n",
    "#             transforms.RandomSizedCrop(224),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.ToTensor(),\n",
    "#             normalize,\n",
    "#         ])),\n",
    "#         batch_size=args.batch_size, shuffle=True,\n",
    "#         num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "#     val_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.ImageFolder(valdir, transforms.Compose([\n",
    "#             transforms.Scale(256),\n",
    "#             transforms.CenterCrop(224),\n",
    "#             transforms.ToTensor(),\n",
    "#             normalize,\n",
    "#         ])),\n",
    "#         batch_size=args.batch_size, shuffle=False,\n",
    "#         num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    # Create dataset and dataloader objects\n",
    "    train_dataset = torch.utils.TensorDataset(X_train_torch, X_train_labels_torch)\n",
    "    train_loader = torch.utils.DataLoader(train_dataset, \n",
    "                                    batch_size=args.batch_size, \n",
    "                                    shuffle=True,\n",
    "                                    num_workers=args.workers, \n",
    "                                    pin_memory=True\n",
    "                                   )\n",
    "    \n",
    "    val_dataset = torch.utils.TensorDataset(X_test_torch, X_test_labels_torch)\n",
    "    val_loader = torch.utils.DataLoader(val_dataset, \n",
    "                                    batch_size=args.batch_size, \n",
    "                                    shuffle=True,\n",
    "                                    num_workers=args.workers, \n",
    "                                    pin_memory=True\n",
    "                                   )\n",
    "\n",
    "\n",
    "    # define loss function (criterion) and pptimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.evaluate:\n",
    "        validate(val_loader, model, criterion)\n",
    "        return\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': args.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        }, is_best)\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(non_blocking=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.cuda(non_blocking=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2E-H4owPLySX"
   },
   "source": [
    "## DCGan features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mZdIQK-PLySZ"
   },
   "source": [
    "GANs correspond to an unsupervised technique for generating images. In https://arxiv.org/pdf/1511.06434.pdf, Sec. 5.1 shows that the representation obtained from the Discriminator has some nice generalization properties on CIFAR10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_O3oPSXFLySb"
   },
   "source": [
    "__Question 6:__  Using for instance a pretrained model from https://github.com/soumith/dcgan.torch combined with https://github.com/pytorch/examples/tree/master/dcgan, propose a model to train on $\\mathcal{X}_{\\text{train}}$. Train it and report its accuracy.\n",
    "\n",
    "*Hint:* You can use the library: https://github.com/bshillingford/python-torchfile to load the weights of a model from torch(Lua) to pytorch(python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHuL_Q9ELySc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uydU5kt6LySf"
   },
   "source": [
    "# Incorporating *a priori*\n",
    "Geometrical *a priori* are appealing for image classification tasks. For now, we only consider linear transformations $\\mathcal{T}$ of the inputs $x:\\mathbb{S}^2\\rightarrow\\mathbb{R}$ where $\\mathbb{S}$ is the support of an image, meaning that:\n",
    "\n",
    "$$\\forall u\\in\\mathbb{S}^2,\\mathcal{T}(\\lambda x+\\mu y)(u)=\\lambda \\mathcal{T}(x)(u)+\\mu \\mathcal{T}(y)(u)\\,.$$\n",
    "\n",
    "For instance if an image had an infinite support, a translation $\\mathcal{T}_a$ by $a$ would lead to:\n",
    "\n",
    "$$\\forall u, \\mathcal{T}_a(x)(u)=x(u-a)\\,.$$\n",
    "\n",
    "Otherwise, one has to handle several boundary effects.\n",
    "\n",
    "__Question 7:__ Explain the issues when dealing with translations, rotations, scaling effects, color changes on $32\\times32$ images. Propose several ideas to tackle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "489zmX-DLySg"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sOY-eddZLySg"
   },
   "source": [
    "## Data augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0pNAt9lLySh"
   },
   "source": [
    "__Question 8:__ Propose a set of geometric transformation beyond translation, and incorporate them in your training pipeline. Train the model of the __Question 3__ and __Question 4__ with them and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNhwJCgELySi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XibVw8EPLySk"
   },
   "source": [
    "## Wavelets\n",
    "\n",
    "__Question 9:__ Use a Scattering Transform as an input to a ResNet-like architecture. You can find a baseline here: https://arxiv.org/pdf/1703.08961.pdf.\n",
    "\n",
    "*Hint:* You can use the following package: https://www.kymat.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoIJZon2LySl"
   },
   "source": [
    "# Weak supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kuc6HTaCLySn"
   },
   "source": [
    "Weakly supervised techniques permit to tackle the issue of labeled data. An introduction to those techniques can be found here: https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html.\n",
    "\n",
    "__(Open) Question 10:__ Pick a weakly supervised method that will potentially use $\\mathcal{X}\\cup\\mathcal{X}_{\\text{train}}$ to train a representation (a subset of $\\mathcal{X}$ is also fine). Evaluate it and report the accuracies. You should be careful in the choice of your method, in order to avoid heavy computational effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9BIQ5THLySo"
   },
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12R2IhoKLySq"
   },
   "source": [
    "__Question 11:__ Write a short report explaining the pros and the cons of each methods that you implemented. 25% of the grade of this project will correspond to this question, thus, it should be done carefully. In particular, please add a plot that will summarize all your numerical results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJLUmc4lLySr"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3-TD_ADL.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
