{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7H2lhj6gLyRU"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeJhO9RPLyRZ"
   },
   "source": [
    "# Small data and deep learning\n",
    "This mini-project proposes to study several techniques for improving challenging context, in which few data and resources are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-ZDU0bjLyRa"
   },
   "source": [
    "# Introduction\n",
    "Assume we are in a context where few \"gold\" labeled data are available for training, say $\\mathcal{X}_{\\text{train}}\\triangleq\\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$, where $N_{\\text{train}}$ is small. A large test set $\\mathcal{X}_{\\text{test}}$ is available. A large amount of unlabeled data, $\\mathcal{X}$, is available. We also assume that we have a limited computational budget (e.g., no GPUs).\n",
    "\n",
    "For each question, write a commented *Code* or a complete answer as a *Markdown*. When the objective of a question is to report a CNN accuracy, please use the following format to report it, at the end of the question:\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
    "|------|------|------|------|\n",
    "|   XXX  | XXX | XXX | XXX |\n",
    "\n",
    "If applicable, please add the field corresponding to the  __Accuracy on Full Data__ as well as a link to the __Reference paper__ you used to report those numbers. (You do not need to train a CNN on the full CIFAR10 dataset)\n",
    "\n",
    "In your final report, please keep the logs of each training procedure you used. We will only run this jupyter if we have some doubts on your implementation. \n",
    "\n",
    "__The total file sizes should not exceed 2MB. Please name your notebook (LASTNAME)\\_(FIRSTNAME).ipynb, zip/tar it with any necessary files required to run your notebook, in a compressed file named (LASTNAME)\\_(FIRSTNAME).X where X is the corresponding extension. Zip/tar files exceeding 2MB will not be considered for grading. Submit the compressed file via the submission link provided on the website of the class.__\n",
    "\n",
    "You can use https://colab.research.google.com/ to run your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b60U5du3LyRb"
   },
   "source": [
    "## Training set creation\n",
    "__Question 1:__ Propose a dataloader or modify the file located at https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py in order to obtain a training loader that will only use the first 100 samples of the CIFAR-10 training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gk5ETsCyLyRc"
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mAGYF_H3LyRd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQmCidX2LyRg"
   },
   "source": [
    "### Import downloader behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "liXruxpqLyRh"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sys\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import hashlib\n",
    "import errno\n",
    "from torch.utils.model_zoo import tqdm\n",
    "\n",
    "## UTILS FUNCTION from https://github.com/pytorch/vision/blob/master/torchvision/datasets/utils.py\n",
    "\n",
    "def check_integrity(fpath, md5=None):\n",
    "    if md5 is None:\n",
    "        return True\n",
    "    if not os.path.isfile(fpath):\n",
    "        return False\n",
    "    md5o = hashlib.md5()\n",
    "    with open(fpath, 'rb') as f:\n",
    "        # read in 1MB chunks\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b''):\n",
    "            md5o.update(chunk)\n",
    "    md5c = md5o.hexdigest()\n",
    "    if md5c != md5:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def download_url(url, root, filename=None, md5=None):\n",
    "    \"\"\"Download a file from a url and place it in root.\n",
    "    Args:\n",
    "        url (str): URL to download file from\n",
    "        root (str): Directory to place downloaded file in\n",
    "        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n",
    "        md5 (str, optional): MD5 checksum of the download. If None, do not check\n",
    "    \"\"\"\n",
    "    from six.moves import urllib\n",
    "\n",
    "    root = os.path.expanduser(root)\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "    fpath = os.path.join(root, filename)\n",
    "\n",
    "    makedir_exist_ok(root)\n",
    "\n",
    "    # downloads file\n",
    "    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n",
    "        print('Using downloaded and verified file: ' + fpath)\n",
    "    else:\n",
    "        try:\n",
    "            print('Downloading ' + url + ' to ' + fpath)\n",
    "            urllib.request.urlretrieve(\n",
    "                url, fpath,\n",
    "                reporthook=gen_bar_updater()\n",
    "            )\n",
    "        except OSError:\n",
    "            if url[:5] == 'https':\n",
    "                url = url.replace('https:', 'http:')\n",
    "                print('Failed download. Trying https -> http instead.'\n",
    "                      ' Downloading ' + url + ' to ' + fpath)\n",
    "                urllib.request.urlretrieve(\n",
    "                    url, fpath,\n",
    "                    reporthook=gen_bar_updater()\n",
    "                )\n",
    "\n",
    "def makedir_exist_ok(dirpath):\n",
    "    \"\"\"\n",
    "    Python2 support for os.makedirs(.., exist_ok=True)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except OSError as e:\n",
    "        if e.errno == errno.EEXIST:\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "def gen_bar_updater():\n",
    "    pbar = tqdm(total=None)\n",
    "\n",
    "    def bar_update(count, block_size, total_size):\n",
    "        if pbar.total is None and total_size:\n",
    "            pbar.total = total_size\n",
    "        progress_bytes = count * block_size\n",
    "        pbar.update(progress_bytes - pbar.n)\n",
    "\n",
    "    return bar_update\n",
    "\n",
    "## CIFAR DATASET DOWNLOADER from https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py\n",
    "    \n",
    "class CIFAR10(data.Dataset):\n",
    "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
    "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
    "            creates from test set.\n",
    "        transform (callable, optional): A function/transform that takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-10-batches-py'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    filename = \"cifar-10-python.tar.gz\"\n",
    "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "    train_list = [\n",
    "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
    "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
    "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
    "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
    "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'batches.meta',\n",
    "        'key': 'label_names',\n",
    "        'md5': '5ff9c542aee3614f3951f8cda6e48888',\n",
    "    }\n",
    "\n",
    "    def __init__(self, root, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        if self.train:\n",
    "            downloaded_list = self.train_list\n",
    "        else:\n",
    "            downloaded_list = self.test_list\n",
    "\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "\n",
    "        # now load the picked numpy arrays\n",
    "        for file_name, checksum in downloaded_list:\n",
    "            file_path = os.path.join(self.root, self.base_folder, file_name)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    entry = pickle.load(f)\n",
    "                else:\n",
    "                    entry = pickle.load(f, encoding='latin1')\n",
    "                self.data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.targets.extend(entry['labels'])\n",
    "                else:\n",
    "                    self.targets.extend(entry['fine_labels'])\n",
    "\n",
    "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
    "        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "\n",
    "        self._load_meta()\n",
    "\n",
    "    def _load_meta(self):\n",
    "        path = os.path.join(self.root, self.base_folder, self.meta['filename'])\n",
    "        if not check_integrity(path, self.meta['md5']):\n",
    "            raise RuntimeError('Dataset metadata file not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "        with open(path, 'rb') as infile:\n",
    "            if sys.version_info[0] == 2:\n",
    "                data = pickle.load(infile)\n",
    "            else:\n",
    "                data = pickle.load(infile, encoding='latin1')\n",
    "            self.classes = data[self.meta['key']]\n",
    "        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        root = self.root\n",
    "        for fentry in (self.train_list + self.test_list):\n",
    "            filename, md5 = fentry[0], fentry[1]\n",
    "            fpath = os.path.join(root, self.base_folder, filename)\n",
    "            if not check_integrity(fpath, md5):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def download(self):\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        download_url(self.url, self.root, self.filename, self.tgz_md5)\n",
    "\n",
    "        # extract file\n",
    "        with tarfile.open(os.path.join(self.root, self.filename), \"r:gz\") as tar:\n",
    "            tar.extractall(path=self.root)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = 'train' if self.train is True else 'test'\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "\n",
    "class CIFAR100(CIFAR10):\n",
    "    \"\"\"`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    This is a subclass of the `CIFAR10` Dataset.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-100-python'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
    "    train_list = [\n",
    "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'meta',\n",
    "        'key': 'fine_label_names',\n",
    "        'md5': '7973b15100ade9c7d40fb424638fde48',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJqbeSk7LyRk"
   },
   "source": [
    "### Run test downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVl9FQvyLyRl"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset\"\n",
    "TRAIN_BATCH_PATH = \"/\".join([DATASET_PATH, \"cifar-10-batches-py\", \"data_batch_1\"])\n",
    "TEST_BATCH_PATH = \"/\".join([DATASET_PATH, \"cifar-10-batches-py\", \"test_batch\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twjlIeo8LyRn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download train dataset\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "print(\"Download train dataset\")\n",
    "downloader_train = CIFAR10(root=DATASET_PATH, train=True, transform=None, target_transform=None, download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXmv8_CGLyRq"
   },
   "source": [
    "### Unpack the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9uBc3Zi4LyRr"
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "train_data_dict = unpickle(TRAIN_BATCH_PATH)\n",
    "test_data_dict = unpickle(TEST_BATCH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8q1Jg_9SLyRv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (100, 32, 32, 3)\n",
      "X_left shape (9900, 32, 32, 3)\n",
      "X_test shape (10000, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def custom_dataset_process(data_dict, n_start = 0, n_end=None):\n",
    "    \"\"\" Load the CIFAR10 data as intended for this project\n",
    "    :param data_dict: dict, dictionnary generated by the unpickle function\n",
    "    :param n_start: integer, default=None, index value of the first elements to take\n",
    "    :param n_end: integer, default=None, index value of the last elements to take\n",
    "    :return x: np.array, data array with shape n*32*32*3, with n number of images, of shape 32*32 in 3 colors\"\"\"\n",
    "    \n",
    "    labels = data_dict[b\"labels\"][n_start:n_end]\n",
    "    data = data_dict[b\"data\"][n_start:n_end]\n",
    "        \n",
    "    les_im = np.array(data)\n",
    "    les_im_reshape = []\n",
    "    for id_im, im in enumerate(les_im):\n",
    "        im_reshape = np.rot90(im.reshape((32, 32, 3), order=\"F\"), k=3)\n",
    "        les_im_reshape.append(im_reshape)\n",
    "    \n",
    "    les_im_reshape = np.array(les_im_reshape)\n",
    "    return les_im_reshape, labels\n",
    "\n",
    "X_train, X_train_labels = custom_dataset_process(train_data_dict, n_start=0, n_end=100)\n",
    "X_left, X_left_labels = custom_dataset_process(train_data_dict, n_start=100, n_end=None)\n",
    "X_test, X_test_labels = custom_dataset_process(test_data_dict)\n",
    "\n",
    "def show_dataset():\n",
    "    to_show = [[\"X_train\", X_train], [\"X_left\", X_left], [\"X_test\", X_test]]\n",
    "    for i, X in enumerate(to_show):\n",
    "        \n",
    "        print(X[0], \"shape\", X[1].shape)\n",
    "        plt.subplot(1, len(to_show), i+1)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(X[1][1])\n",
    "\n",
    "show_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8siNgFNLyRz"
   },
   "source": [
    "This is our dataset $\\mathcal{X}_{\\text{train}}$, it will be used until the end of this project. The remaining samples correspond to $\\mathcal{X}$. The testing set $\\mathcal{X}_{\\text{test}}$ corresponds to the whole testing set of CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXn5rOGkLyR3"
   },
   "source": [
    "## Testing procedure\n",
    "__Question 2:__ Explain why the evaluation of the training procedure is difficult. Propose several solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0o251Kz8LyR4"
   },
   "source": [
    "Evaluation of the training procedure may be difficult because of the low number of labeled data we are using for the model training. We may have quite a hard time resolving overfitting problems: we have few gold data.\n",
    "\n",
    "As seen in the class, solutions exists:\n",
    "- We can use semi-supervized learning to try to make guesses about unlabeled train data\n",
    "- Use transfer learning, and just fine-tune our model without the need of a huge great dataset\n",
    "- Weak supervision or traditional supervision are also existing solutions, but we will not use them as they involve to ask for a new labelling round, even at higher/abstract level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tp4Hh9BoLyR5"
   },
   "source": [
    "# Raw approach: the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsyOxEtxLyR6"
   },
   "source": [
    "In this section, the goal is to train a CNN on $\\mathcal{X}_{\\text{train}}$ and compare its performances with reported number from the litterature. You will have to re-use and/or design a standard classification pipeline. You should optimize your pipeline to obtain the best performances (image size, data augmentation by flip, ...).\n",
    "\n",
    "The key ingredients for training a CNN are the batch size, as well as the learning rate schedule, i.e. how to decrease the learning rate as a function of the number of epochs. A possible schedule is to start the learning rate at 0.1 and decreasing it every 30 epochs by 10. In case of divergence, reduce the laerning rate. A potential batch size could be 10, yet this can be cross-validated.\n",
    "\n",
    "You can get some baselines accuracies in this paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf. Obviously, it is a different context, as those researchers had access to GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-C45ofQLyR7"
   },
   "source": [
    "## ResNet architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tv78YLvaLyR8"
   },
   "source": [
    "__Question 3:__ Write a classification pipeline for $\\mathcal{X}_{\\text{train}}$, train from scratch and evaluate a *ResNet-18* architecture specific to CIFAR10 (details about the ImageNet model can be found here: https://arxiv.org/abs/1409.1556 ). If possible, please report the accuracy obtained on the whole dataset, as well as the reference paper/GitHub link you might have used.\n",
    "\n",
    "*Hint:* You can re-use the following code: https://github.com/kuangliu/pytorch-cifar. During a training of 10 epochs, a batch size of 10 and a learning rate of 0.01, one obtains 40% accuracy on $\\mathcal{X}_{\\text{train}}$ (~2 minutes) and 20% accuracy on $\\mathcal{X}_{\\text{test}}$ (~5 minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7iDpQVyBLySG"
   },
   "source": [
    "## VGG-like architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwkHhRzKLySH"
   },
   "source": [
    "__Question 4:__ Same question as before, but with a *VGG*. Which model do you recommend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0UnhQVqLySI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Hij8vN4LySL"
   },
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ta0dVVkLLySM"
   },
   "source": [
    "We propose to use pre-trained models on a classification and generative task, in order to improve the results of our setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pf28XHgMLySO"
   },
   "source": [
    "## ImageNet features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yO8O4EBLySP"
   },
   "source": [
    "Now, we will use some pre-trained models on ImageNet and see how well they compare on CIFAR. A list is available on: https://pytorch.org/docs/stable/torchvision/models.html.\n",
    "\n",
    "__Question 5:__ Pick a model from the list above, adapt it to CIFAR and retrain its final layer (or a block of layers, depending on the resources to which you have access to). Report its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JeHJQDBhLySS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_names ['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'inception_v3', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn']\n",
      "=> creating model 'resnet18'\n",
      "Epoch: [0][0/7]\tTime 4.358 (4.358)\tData 0.215 (0.215)\tLoss 6.6142 (6.6142)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:205: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/paul/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:206: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/625]\tTime 0.386 (0.386)\tLoss 15184.2051 (15184.2051)\tPrec@1 18.750 (18.750)\tPrec@5 50.000 (50.000)\n",
      "Test: [10/625]\tTime 0.177 (0.198)\tLoss 14560.8340 (15332.8338)\tPrec@1 12.500 (14.205)\tPrec@5 37.500 (48.864)\n",
      "Test: [20/625]\tTime 0.176 (0.189)\tLoss 14852.1875 (15532.4990)\tPrec@1 18.750 (12.500)\tPrec@5 62.500 (50.893)\n",
      "Test: [30/625]\tTime 0.178 (0.185)\tLoss 15995.5029 (15987.6265)\tPrec@1 6.250 (10.887)\tPrec@5 37.500 (49.597)\n",
      "Test: [40/625]\tTime 0.176 (0.183)\tLoss 14907.1152 (15810.4694)\tPrec@1 6.250 (11.738)\tPrec@5 62.500 (50.762)\n",
      "Test: [50/625]\tTime 0.177 (0.182)\tLoss 22980.0918 (16209.4238)\tPrec@1 0.000 (11.642)\tPrec@5 43.750 (49.755)\n",
      "Test: [60/625]\tTime 0.176 (0.184)\tLoss 21687.6230 (16432.9422)\tPrec@1 25.000 (11.270)\tPrec@5 37.500 (49.590)\n",
      "Test: [70/625]\tTime 0.177 (0.183)\tLoss 13861.6162 (16118.9957)\tPrec@1 12.500 (11.708)\tPrec@5 56.250 (50.352)\n",
      "Test: [80/625]\tTime 0.176 (0.183)\tLoss 13133.5918 (16219.5702)\tPrec@1 6.250 (11.420)\tPrec@5 43.750 (49.537)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "print(\"model_names\", model_names)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, data=\"dataset\", arch=\"resnet18\", workers=4, epochs=99, start_epoch=0, batch_size=16, lr=0.1, momentum=0.9, weight_decay=1e-4, print_freq=10, resume=\"\", evaluate=False, pretrained=False):\n",
    "        self.data = data\n",
    "        self.arch = arch\n",
    "        self.workers = workers\n",
    "        self.epochs = epochs\n",
    "        self.start_epoch = start_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.print_freq = print_freq\n",
    "        self.resume = resume\n",
    "        self.evaluate = evaluate\n",
    "        self.pretrained = pretrained\n",
    "\n",
    "\n",
    "class MyTensorDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset wrapping tensors.\n",
    "\n",
    "    Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "\n",
    "    Arguments:\n",
    "        *tensors (Tensor): tensors that have the same size of the first dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_tensor, Y_tensor, transform):\n",
    "        assert len(X_tensor) == len(Y_tensor)\n",
    "        self.X_tensor = X_tensor\n",
    "        self.Y_tensor = Y_tensor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple([self.transform(self.X_tensor[index]), self.Y_tensor[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_tensor)\n",
    "        \n",
    "        \n",
    "best_prec1 = 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    global args, best_prec1\n",
    "#     args = parser.parse_args()\n",
    "    args = Args()\n",
    "\n",
    "    # create model\n",
    "    if args.pretrained:\n",
    "        print(\"=> using pre-trained model '{}'\".format(args.arch))\n",
    "        model = models.__dict__[args.arch](pretrained=True)\n",
    "    else:\n",
    "        print(\"=> creating model '{}'\".format(args.arch))\n",
    "        model = models.__dict__[args.arch]()\n",
    "\n",
    "    if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):\n",
    "        model.features = torch.nn.DataParallel(model.features)\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Data loading code\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    transf = transforms.Compose(\n",
    "        [transforms.ToPILImage(),\n",
    "         transforms.CenterCrop(224),\n",
    "         transforms.ToTensor(),\n",
    "         normalize\n",
    "        ])\n",
    "    \n",
    "    # Train dataset \n",
    "    train_dataset =  MyTensorDataset(X_train, X_train_labels, transform=transf)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=args.batch_size, shuffle=True,\n",
    "        num_workers=args.workers, pin_memory=True)\n",
    "    \n",
    "    # Validation dataset\n",
    "    val_dataset =  MyTensorDataset(X_test, X_test_labels, transform=transf)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=args.batch_size, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    # define loss function (criterion) and pptimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.evaluate:\n",
    "        validate(val_loader, model, criterion)\n",
    "        return\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': args.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        }, is_best)\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(non_blocking=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        \n",
    "#         losses.update(loss.data[0], input.size(0))\n",
    "#         top1.update(prec1[0], input.size(0))\n",
    "#         top5.update(prec5[0], input.size(0))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.cuda(non_blocking=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], input.size(0))\n",
    "#         top1.update(prec1[0], input.size(0))\n",
    "#         top5.update(prec5[0], input.size(0))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2E-H4owPLySX"
   },
   "source": [
    "## DCGan features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mZdIQK-PLySZ"
   },
   "source": [
    "GANs correspond to an unsupervised technique for generating images. In https://arxiv.org/pdf/1511.06434.pdf, Sec. 5.1 shows that the representation obtained from the Discriminator has some nice generalization properties on CIFAR10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_O3oPSXFLySb"
   },
   "source": [
    "__Question 6:__  Using for instance a pretrained model from https://github.com/soumith/dcgan.torch combined with https://github.com/pytorch/examples/tree/master/dcgan, propose a model to train on $\\mathcal{X}_{\\text{train}}$. Train it and report its accuracy.\n",
    "\n",
    "*Hint:* You can use the library: https://github.com/bshillingford/python-torchfile to load the weights of a model from torch(Lua) to pytorch(python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHuL_Q9ELySc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uydU5kt6LySf"
   },
   "source": [
    "# Incorporating *a priori*\n",
    "Geometrical *a priori* are appealing for image classification tasks. For now, we only consider linear transformations $\\mathcal{T}$ of the inputs $x:\\mathbb{S}^2\\rightarrow\\mathbb{R}$ where $\\mathbb{S}$ is the support of an image, meaning that:\n",
    "\n",
    "$$\\forall u\\in\\mathbb{S}^2,\\mathcal{T}(\\lambda x+\\mu y)(u)=\\lambda \\mathcal{T}(x)(u)+\\mu \\mathcal{T}(y)(u)\\,.$$\n",
    "\n",
    "For instance if an image had an infinite support, a translation $\\mathcal{T}_a$ by $a$ would lead to:\n",
    "\n",
    "$$\\forall u, \\mathcal{T}_a(x)(u)=x(u-a)\\,.$$\n",
    "\n",
    "Otherwise, one has to handle several boundary effects.\n",
    "\n",
    "__Question 7:__ Explain the issues when dealing with translations, rotations, scaling effects, color changes on $32\\times32$ images. Propose several ideas to tackle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "489zmX-DLySg"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sOY-eddZLySg"
   },
   "source": [
    "## Data augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0pNAt9lLySh"
   },
   "source": [
    "__Question 8:__ Propose a set of geometric transformation beyond translation, and incorporate them in your training pipeline. Train the model of the __Question 3__ and __Question 4__ with them and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNhwJCgELySi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XibVw8EPLySk"
   },
   "source": [
    "## Wavelets\n",
    "\n",
    "__Question 9:__ Use a Scattering Transform as an input to a ResNet-like architecture. You can find a baseline here: https://arxiv.org/pdf/1703.08961.pdf.\n",
    "\n",
    "*Hint:* You can use the following package: https://www.kymat.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoIJZon2LySl"
   },
   "source": [
    "# Weak supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kuc6HTaCLySn"
   },
   "source": [
    "Weakly supervised techniques permit to tackle the issue of labeled data. An introduction to those techniques can be found here: https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html.\n",
    "\n",
    "__(Open) Question 10:__ Pick a weakly supervised method that will potentially use $\\mathcal{X}\\cup\\mathcal{X}_{\\text{train}}$ to train a representation (a subset of $\\mathcal{X}$ is also fine). Evaluate it and report the accuracies. You should be careful in the choice of your method, in order to avoid heavy computational effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9BIQ5THLySo"
   },
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12R2IhoKLySq"
   },
   "source": [
    "__Question 11:__ Write a short report explaining the pros and the cons of each methods that you implemented. 25% of the grade of this project will correspond to this question, thus, it should be done carefully. In particular, please add a plot that will summarize all your numerical results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJLUmc4lLySr"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3-TD_ADL.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
